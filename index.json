[{"content":"This note details the process of creating and configuring a development environment with CUDA support using Docker, from initializing the container and installing the necessary packages such as vim, nano, git, openssh-server, etc., to configuring SSH to allow root to log in and set the root password, and configuring the nvcc environment variable for the user to ensure that the CUDA toolchain is available.\nIt also describes how to clear the command history to maintain privacy and save the configured container as a new image version (4.0).\nThe documentation then describes the process of creating a new user based on this customized image, including adding the user to the host, obtaining his or her UID and GID, and then starting a new Docker container with this information while mapping the user\u0026rsquo;s home directory inside the container.\nFinally, the note mention changing the ownership of the user\u0026rsquo;s folder to the newly created user and suggest future improvements that can be made to automate this series of steps by writing Dockerfiles and scripts. The entire process demonstrates the complete flow of building a Docker development environment from scratch suitable for deep learning or other GPU-accelerated tasks.\n修改docker权限 1 newgrp docker 从原始容器中创建模板 创建并进入容器 1 2 3 docker run --gpus all -itd --name username --hostname host-57 -p 10023:22 dockerpull.org/nvidia/cuda:12.6.3-cudnn-devel-ubuntu24.04 docker exec -it username /bin/bash 更新并安装包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apt update apt install vim 5 69 apt install nano apt install git apt install openssh-server apt install sudo apt install python-is-python3 apt install python3-pip apt install iputils-ping apt install --reinstall ubuntu-standard 删除unminimize提示 1 rm -f /etc/update-motd.d/60-unminimize 修改ssh 1 2 3 vim /etc/ssh/sshd_config PermitRootLogin yes 设置root密码 1 host12345 配置用户nvcc环境 1 vim /etc/skel/.bashrc 1 2 3 4 5 # Set up CUDA environment variables if [ -d /usr/local/cuda/bin ]; then export PATH=/usr/local/cuda/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} fi 清除历史记录 1 2 3 echo -n \u0026#34;\u0026#34; \u0026gt; ~/.bash_history history -r history -c 保存容器到镜像(4.0) 1 2 3 docker ps docker commit -a cjy -m 保存容器到镜像 docker_id newstudent:4.0 从模板中创建新用户 在主机中创建新用户 1 adduser --home /media/disk/home/username username 获取用户UID和GID 1 id username 从模板中创建容器并进入 1 2 3 docker run -u uid:gid --gpus all -itd --name username --hostname host-57 -p 10000:22 -p 10001-10009:10001-10009 -v /media/disk/home/username:/home/username newstudent:4.0 docker exec -it username /bin/bash 更改用户的UID和GID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 su # 创建用户组GID groupadd -g gid username # 创建用户 adduser --uid uid --gid gid username # 复制.bashrc到用户home目录 cp -r /etc/skel/. /home/lxy/ # 重启ssh service ssh restart # 清除历史记录 echo -n \u0026#34;\u0026#34; \u0026gt; ~/.bash_history history -r history -c # 切换到用户 su username # 清除历史记录 echo -n \u0026#34;\u0026#34; \u0026gt; ~/.bash_history history -r history -c 将用户文件夹所有者从root修改为用户自身 1 2 # 一般文件权限755，文件夹权限644 chown -R 1003:1003 /home/username TODO: 使用Dockerfile和脚本化 ","permalink":"https://cjy1018.github.io/posts/2024-12-04-docker-new-student/","summary":"\u003cp\u003eThis note details the process of creating and configuring a development environment with CUDA support using Docker, from initializing the container and installing the necessary packages such as vim, nano, git, openssh-server, etc., to configuring SSH to allow root to log in and set the root password, and configuring the nvcc environment variable for the user to ensure that the CUDA toolchain is available.\u003c/p\u003e\n\u003cp\u003eIt also describes how to clear the command history to maintain privacy and save the configured container as a new image version (4.0).\u003c/p\u003e","title":"Docker New Student"},{"content":"Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nHugo is a static site generator written in Go, optimized for speed and designed for flexibility.\nHugo is a static site generator written in Go, optimized for speed and designed for flexibility. With its advanced templating system and fast asset pipelines, Hugo renders a complete site in seconds, often less.\nDue to its flexible framework, multilingual support, and powerful taxonomy system, Hugo is widely used to create:\nCorporate, government, nonprofit, education, news, event, and project sites Documentation sites Image portfolios Landing pages Business, professional, and personal blogs Resumes and CVs 启动服务 1 hugo server 创建帖子 1 2 hugo new posts/2024-12-03-hugo/index.md hugo new content content/posts/xxxxx.md 部署到 Github 1 hugo --baseURL https://cjy1018.github.io/ 目录结构 post 模板文件 1 ├─archetypes default.md\n1 2 3 4 5 6 --- date: \u0026#39;{{ .Date }}\u0026#39; title: {{ title }} author: \u0026#39;CJY\u0026#39; tags: [] --- 代码样式 1 2 3 ├─assets │ └─css │ └─extended assets\\css\\extended\\blank.css\n1 2 3 4 5 6 .post-content pre, code { font-family: \u0026#34;consolas\u0026#34;, monospace; font-size: 20px; line-height: 1.2; } archives, faq, search, space 模板文件; 帖子目录 1 2 3 4 5 ├─content │ └─posts # 帖子目录 │ ├─2024-11-20-llama-omni │ ├─2024-11-20-whisper │ └─2024-12-03-hugo 公开目录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ├─public │ ├─archives │ ├─assets │ │ ├─css │ │ └─js │ ├─categories │ ├─faq │ ├─page │ │ └─1 │ ├─posts │ │ ├─2024-11-20-llama-omni │ │ ├─2024-11-20-whisper │ │ ├─2024-12-03-hugo │ │ └─page │ │ └─1 │ ├─search │ ├─space │ └─tags │ ├─asr │ │ └─page │ │ └─1 │ └─skill │ └─page │ └─1 PaperMod 主题文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 └─themes └─PaperMod ├─.github │ ├─ISSUE_TEMPLATE │ └─workflows ├─assets │ ├─css │ │ ├─common │ │ ├─core │ │ ├─extended │ │ └─includes │ └─js ├─i18n ├─images └─layouts ├─partials* │ └─templates │ └─_funcs ├─shortcodes └─_default └─_markup themes\\PaperMod\\layouts\\partials\\post_meta.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 {{- $scratch := newScratch }} {{- if not .Date.IsZero -}} {{- $scratch.Add \u0026#34;meta\u0026#34; (slice (printf \u0026#34;\u0026lt;span title=\u0026#39;%s\u0026#39;\u0026gt;Date: %s\u0026lt;/span\u0026gt;\u0026#34; (.Date) (.Date | time.Format (default \u0026#34;January 2, 2006\u0026#34; site.Params.DateFormat)))) }} {{- end }} {{- if (.Param \u0026#34;ShowReadingTime\u0026#34;) -}} {{- $translatedReadTime := i18n \u0026#34;read_time\u0026#34; .ReadingTime }} {{- $scratch.Add \u0026#34;meta\u0026#34; (slice (printf \u0026#34;Estimated Reading Time: %s\u0026#34; $translatedReadTime | default (printf \u0026#34;%d min\u0026#34; .ReadingTime))) }} {{- end }} {{- if not (.Param \u0026#34;hideAuthor\u0026#34;) -}} {{- with (partial \u0026#34;author.html\u0026#34; .) }} {{- $scratch.Add \u0026#34;meta\u0026#34; (slice (printf \u0026#34;Author: %s\u0026#34; . | safeHTML)) }} {{- end }} {{- end }} {{- with ($scratch.Get \u0026#34;meta\u0026#34;) }} {{- delimit . \u0026#34;\u0026amp;nbsp;|\u0026amp;nbsp;\u0026#34; | safeHTML -}} {{- end -}} 辅助应用 PaperMod 主题\ngit / github 管理\nPicGo 图床（ip:15000/39000 /picgo）\ngiscus 评论区\nflaticon 矢量图\n教程链接 SonnyCalcr\u0026rsquo;s Blog 教学视频\n","permalink":"https://cjy1018.github.io/posts/2024-12-03-hugo/","summary":"\u003cp\u003eHugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\u003c/p\u003e\n\u003cp\u003eHugo is a static site generator written in Go, optimized for speed and designed for flexibility.\u003c/p\u003e\n\u003cp\u003eHugo is a \u003ca href=\"https://en.wikipedia.org/wiki/Static_site_generator\"\u003estatic site generator\u003c/a\u003e written in \u003ca href=\"https://go.dev/\"\u003eGo\u003c/a\u003e, optimized for speed and designed for flexibility. With its advanced templating system and fast asset pipelines, Hugo renders a complete site in seconds, often less.\u003c/p\u003e","title":"Hugo"},{"content":"LLAMA-OMNI: SEAMLESS SPEECH INTERACTION WITH LARGE LANGUAGE MODELS Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8BInstruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.\n","permalink":"https://cjy1018.github.io/posts/2024-11-20-llama-omni/","summary":"\u003ch2 id=\"llama-omni-seamless-speech-interaction--with-large-language-models\"\u003eLLAMA-OMNI: SEAMLESS SPEECH INTERACTION  WITH LARGE LANGUAGE MODELS\u003c/h2\u003e\n\u003cp\u003eModels like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8BInstruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.\u003c/p\u003e","title":"Llama Omni"},{"content":"Robust Speech Recognition via Large-Scale Weak Supervision We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\nFigure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets. (Image source: Radford et al. 2022) Tokenizer tiktoken库\n","permalink":"https://cjy1018.github.io/posts/2024-11-20-whisper/","summary":"\u003ch2 id=\"robust-speech-recognition-via-large-scale-weak-supervision\"\u003eRobust Speech Recognition via Large-Scale Weak Supervision\u003c/h2\u003e\n\u003cp\u003eWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\u003c/p\u003e","title":"Whisper"},{"content":"","permalink":"https://cjy1018.github.io/faq/","summary":"FAQ","title":"FAQ"},{"content":"","permalink":"https://cjy1018.github.io/space/","summary":"Space","title":"Space"}]